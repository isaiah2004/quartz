# Agent Foundational Model

They propose the below framework.

![[Pasted image 20240220014340.png]]

*Figure 3. Overview of our Interactive Agent framework. Our foundation model is designed to proce

 multi-modal information that conveys various levels of abstraction. This approach facilitates a comprehensive understanding of the context and environment, thus ensuring that actions are coherent. By training on a variety of task domains and applications, we develop a versatile foundation model that can be fine-tuned for executing optimal actions in a variety of contexts, paving the way towards generally intelligent agents.*

By combining [[Visual perception]] and linguistic understanding. We can achieve a more intuitive understanding of their environment. --> Better contextual reasoning. 

Their work is on developing a joint image and video encoder and aligning this joint encoder with existing [[Foundation Modals]]

This has several notable benefits:

1. It allows for the use of both action, image and video with language.
2. Increases the capabilities of the model across a variety of downstream tasks. 
3. Decrease Overall model size.
4. Smaller size allows for edge deployments of modals.

## Model Architecture

> To effectively initialize our model to handle text, visual, and agent tokens as input, we initialize our architecture with two pre-trained submodules. First, we use CLIP ViT-B16 from (Radford et al., 2021) to initialize our visual encoder, denoted Eθ, and initialize our action and language model, Fϕ, from OPT-125M (Zhang et al., 2022).

Two models to handle text and visual tokens:
1. [[CLIP ViT-B16]]
2. [[OPT-125M]] 

First they encode each frame in video V<sub>i</sub> as visual features. Z<sub>i</sub>  = E<sub>θ</sub>(V<sub>i</sub>). We enable cross-modal information sharing by training an additional layer $l$  that transforms the embeddings of our visual encoder $E_\theta$  ==>to Token embedding space of Transformer Model $F_\phi$. 

Thus we can obtain $\hat A$,  a text token or Action token.

The details and $\hat A$  at the time t are in paper.

In practice due to memory constraints we can only handle M action and Frames.


To boost training of visual encoder to predict masked visual tokens, we use sinusoidal positional embeddings.
(look into this later)
(He et al., 2022)

"Since we are using relatively small checkpoints, we are able to jointly train our entire model during pre-training, unlike previous visual-language models that largely rely upon frozen submodules and seek to learn an adaptation network for cross-modal alignment"

We train the entire net instead of just an adaptation net.

![[Pasted image 20240220030333.png]]


Every Sample $S$ is denoted as
![[Pasted image 20240220133441.png]]
where,
$W$ is the sequence of tokes corresponding to the text instruction 
$V_i$ is the sequence of image patches on frame $i$
$A_i$ is the sequence of Action Tokens on frame $i$
$i$ is a frame of Video with $T$ Frames. We denote $w_j$ as the tokens of the text prompt $W$.  

The parameters of the model are donated as $\theta$. 

For every sample there are three components to the loss function :
1. Language modeling 
2. Masked-Image auto-encoding
3. action modeling

The Language Modeling loss is a standard causual language modeling loss to minimize the negative log likelihood of each token in the instruction conditioned on prior tokes. 

$$L_{\text{lang}}(S) = - \sum_{j=1}^{|W|} \log p_{\theta}(w_j | w_{<j}).$$

The masked Image auto-encoding loss is generated by random making 75% of patches.

The calculating the mean squared error between the reconstructed image and original image.

The masked encoder loss of sample $S$ is:

$$L_{\text{mae}}(S) = \sum_{t=1}^{T} \left\| U(V_t) - U \left( D_{\theta} \left( E_{\theta}(M(V_t)) \right) \right) \right\|_2^2,$$
finally the action modeling loss minimizes the negative log- likelihood of each action token conditioned on al prior information, including all text tokens, prior visual tokens, and prior action tokens. The action modeling loss for a particular sample $S$ is:

$$L_{\text{act}}(S) = - \sum_{t=1}^{T} \sum_{i=1}^{|A_t|} \log p_{\theta}((a_t)_i | W, V_{\leq t}, A_{\leq t}, (a_t)_{<i}).$$

