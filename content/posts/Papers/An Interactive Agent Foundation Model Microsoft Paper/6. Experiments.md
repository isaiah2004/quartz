## pre-training experiments 
A full 100 epoch training session using the following  datasets:
Language Table, CALVIN, Minecraft, and Bleeding Edge

[[CLIP ViT-B16]] with patch size 16
with [[OPT-125M]]

on 12 nodes of 16 V100 GPUS for 175 hours....

(very long)

### gaming
They sampled input controls as follows or minecraft and bleeding edge:

"For Minecraft, there are additionally 23 button actions, and we discretized mouse actions to 100 bins along the x axis and 100 bins along the y axis. For Bleeding Edge, there are 11 button actions, and 2 joysticks. Each joystick has 256 possible values for rotation and 4 values for magnitude, resulting in a total of 520 joystick action tokens"

### Robotics

here the tokenized all valid actions and added them along agent state.
For all Robotics dat they included a special action token to indicate. the end of trajectory. 

In Language table they made 21 binned actions for each $x$ and $y$ directions. The same was done for end effector translation and previous bot action too. 

In CALVIN the added two actions for gripper 21 actions for every DOF. They also added 1 attributs of the proprioceptive state. 


### total 

> Our gaming dataset has 525,309 trajectories for Minecraft and 256,867 for Bleeding Edge, each consisting of 9 frames. Our robotics dataset consists of 1,233,659 trajectories for Language-Table and 360,566 for CALVIN, each consisting of 4 frames. Therefore, our total dataset consists of 13,416,484 frames.

## Robotics Experiments 
table1
![[Pasted image 20240221012153.png]]
  
The text describes the fine-tuning and evaluation of a pre-trained model on two datasets: the Language Table and CALVIN. For fine-tuning, the model followed the original pre-training pipeline, maintaining the mean absolute error (MAE) and language-modeling loss functions, and the original vocabulary size. During fine-tuning, 50% of image patches were masked, but no masking occurred during evaluation.

For the Language Table dataset, data from a setup with 8 blocks (6 unrelated to the tasks) resulted in 181,020 trajectories. These were split into 4 frames each, yielding 1,233,659 samples for fine-tuning. The model was evaluated on five subtasks related to moving blocks, with performance measured by the average success rate of 50 randomly sampled trajectories across three evaluations. The pre-trained model showed improvement over training from scratch but was outperformed by models like Brohan et al. (2023), possibly due to using less pre-training data.

For the CALVIN dataset, long-step trajectories were divided into 4 frames, creating 360,566 samples across 34 tasks for fine-tuning. A third-person view RGB camera was chosen for image input, and all appearance settings were included for a larger dataset, following the ABCD â†’ D task definition. The model's performance was evaluated by averaging the success rate at each step, showing better results than Multi-context Imitation Learning (MCIL) (Lynch & Sermanet, 2021) while using only 1% of the data.
## Gaming Experiments 
table2
![[Pasted image 20240221012202.png]]

  
The text discusses the evaluation of a model's performance in predicting actions from video frames and high-level instructions in the gaming contexts of Minecraft and Bleeding Edge. The evaluation utilized a held-out test dataset of 100 videos for each game, formatted similarly to the training data. The model's performance was measured using BLEU-4 scores for action predictions, and the results were reported in Table 2. The study compared a baseline pre-trained model against versions fine-tuned on task-specific data, one of which was initialized from the model itself and another from CLIP and OPT. Both fine-tuned models showed a tendency to overfit the training data within 5 epochs, leading to the decision to report BLEU-4 scores from the checkpoints with the highest validation score. The findings indicated that fine-tuning the pre-trained model significantly outperforms training from scratch in both gaming domains, underscoring the value of a diverse pre-training mixture. Additionally, the text mentions visualizations of predicted actions from the fine-tuned model alongside the validation ground-truth, presented in Table 3 and Appendix E, to further illustrate the model's performance.

![[Pasted image 20240221012736.png]]


## Healthcare Experiments 
![[Pasted image 20240221012755.png]]

**Healthcare Setting Evaluation:**

- For visual question-answering, the model uses the question as the text prompt and a fixed text prompt "A video of" for video captioning. The model is trained to predict the text tokens of the caption or answer, with performance measured by the average perplexity across both tasks.
- RASS score prediction is approached as a 10-way activity classification problem, with a separate classification head trained for this purpose. The visual encoder, which takes 9 frames as input, is described in further detail in Appendix A.
- The effectiveness of the pre-training framework is benchmarked against three baselines that use CLIP and OPT for initialization:
    1. A frozen baseline where the pre-trained models are kept frozen while fine-tuning a single linear layer for cross-modal information passing.
    2. A joint baseline where the pre-trained models are fine-tuned jointly with the linear layer. This method involves encoding frames individually with CLIP and concatenating the frame-level embeddings.
    3. A baseline using the same architecture and video-level encoder, initialized from CLIP and OPT but without large-scale agent pre-training.

The model's performance is showcased in Table 4, compared against these proposed baselines. All models were trained for 20 epochs on four 16GB V100 GPUs, with a fixed learning rate of 4e-5, and evaluated on a held-out set without any additional hyperparameter optimization to ensure a fair comparison.



> [!NEXT] NEXT 
> [[7. Conclusion and thoughts]]



