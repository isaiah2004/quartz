## Introduction

>The development of AI systems that can not only gather useful sensory information, but also interact with their environments in meaningful ways has been a long-time goal for AI researchers.

Advantages of Such models:
1. Training across various domains is very scalable
   (A generalist agent. Transactions on Machine Learning Research, 2022.)
2. the AI community has a new set of tools for developing generalist, action-taking AI systems
3. They posit that one of the key reasons why foundation models hallucinate is due to  lack of grounding in the environments. (e.g., large-scale internet data instead of physical or virtual environments)
4. The current dominant approach for building multi-modal systems is to take pretrained foundation models for each modality and then train layers that allow for cross-modal information passing. ( this allows for hallucinations in the original models to carry over to multimodal system)

To solve this problem they suggest developing a unified pretraining framework for handling text, visual data and action as input. We treat each input type as a separate token and pre-train our model to predict masked tokens across all three modalities. Their  approach uses pre-trained language and pre-trained visual-language models to effectively initialize our model an Interactive with humans and its environment. 

They used a 277M model to achieve this. Trained on 13.4 M video frames of data. From various data sources. Successfully engages in multi-modal settings using text, video, images, dialogue, captioning, etc. within four disparate virtual environments. 

(they are currently developing an even larger model.)

The three distinct domains are:
1. Robotics
2. Gaming AI
3. Healthcare


